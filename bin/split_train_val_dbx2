#!/usr/bin/env python
import os
import sys
import pickle
import copy

import argparse
import random
import pandas as pd

import numpy as np
import networkx as nx

from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit

from dockbox2.dbxconfig import no_features

parser = argparse.ArgumentParser(description="split train and validation sets for training with DockBox2")

parser.add_argument('-g',
    dest='pkfile',
    metavar='STR',
    required=True,
    help="pickle input file containing all the graphs")

parser.add_argument('--binarize',
    dest='binarize',
    nargs='?',
    const=0.0,
    type=float,
    help='binarize pkd values using provided value as cutoff (default cutoff: 0.0)')

parser.add_argument('--blacklist',
    dest='blackfile',
    metavar='STR',
    default=None,
    help='provide list of IDs that should not be included!')

parser.add_argument('--cutoff',
    dest='cutoff_correct_pose',
    metavar='FLOAT',
    default=2.0,
    type=float,
    help='RMSD cutoff used to assign correctness label')

parser.add_argument('--normalize',
    dest='normalize',
    action="store_true",
    default=False,
    help='normalize known scoring functions')

parser.add_argument('--nsplits',
    dest='nsplits',
    metavar='INT',
    default=1,
    type=int,
    help='number of splits used for cross-validation')

parser.add_argument('--nullify',
    dest='nullify',
    action="store_true",
    default=False,
    help='nullify pkd when no correct poses were generated!')

parser.add_argument('--maxnodes',
    dest='max_nnodes',
    metavar='INT',
    type=int,
    help='maximum number of poses per graph in training set')

parser.add_argument('--test',
    dest='value_test',
    metavar='FLOAT/STR',
    default='0.0',
    help='fraction assigned to test set or filename including IDs')

parser.add_argument('--train',
    dest='fraction_train',
    metavar='FLOAT',
    default=0.5,
    type=float,
    help='fraction assigned to train set (used when cv is disabled)')

parser.add_argument('--seed',
    dest='seed',
    default=None,
    metavar='INT',
    type=int,
    help='random seed')

# update parser with arguments
args = parser.parse_args()

if args.seed is not None:
    seed = args.seed
else:
    seed = random.randint(0, 1e10)
    print("The random seed is set to %i"%seed)

# set random seed
random.seed(seed)

def is_float(string):
    try:
        float(string)
        return True
    except ValueError:
        return False

with open(args.pkfile, "rb") as ff:
    graphs = pickle.load(ff)

# get PDBIDs from dictionnary keys
pdbids = sorted(list(set(graphs.keys())))

if args.blackfile:
    with open(args.blackfile, 'r') as ff:
       blacklist = ff.readlines()
       blacklist = [pdbid.replace('\n', '') for pdbid in blacklist]

    pdbids = [pdbid for pdbid in pdbids if pdbid not in blacklist]
npdbids = len(pdbids)

if all([isinstance(graphs[pdbid], list) and len(graphs[pdbid])==2 for pdbid in pdbids]):
    pkd = {pdbid: graphs[pdbid][1] for pdbid in pdbids} 
    graphs = {pdbid: graphs[pdbid][0] for pdbid in pdbids}

    print("pKd's values found...")
else:
    pkd = None

if pkd is not None and (args.nullify or args.binarize is not None):
    for pdbid in pdbids:
        G = graphs[pdbid]

        if args.nullify:
            no_correct_pose = True
            for node, data in G.nodes(data=True):
                if data['rmsd'] <= args.cutoff_correct_pose:
                    no_correct_pose = False
                    break

            if no_correct_pose:
                pkd[pdbid] = 0.0

        if args.binarize is not None:
            pkd[pdbid] = 1 if pkd[pdbid] > args.binarize else 0

# get names of scoring functions and other feats
for idx, pdbid in enumerate(pdbids):
    G = graphs[pdbid]

    for node, data in G.nodes(data=True):
        if idx == 0:
            data_names = data.keys()
            scoring_functions = [ft for ft in data_names if ft not in no_features]
        else:
            if any([key not in data_names for key in data]):
                print("Provided graphs are not consistent, %s data not found in every node"%key)  
                sys.exit(1)

# check restrictions on test set
if is_float(args.value_test):
    fraction_test = float(args.value_test)

    ntest = int(fraction_test * npdbids)
    pdbids_test = sorted(random.sample(pdbids, ntest))
else:
    with open(args.value_test, 'r') as ff:
        pdbids_test = ff.readlines()
        pdbids_test = [pdbid.replace('\n', '') for pdbid in pdbids_test]

    pdbids_test = sorted([pdbid for pdbid in pdbids if pdbid in pdbids_test])

pdbids_train_val = []
labels_train_val = []

for pdbid in pdbids:
    G = graphs[pdbid]

    node_labels = []
    for node, data in G.nodes(data=True):
        # set label from cutoff_correct_pose
        if data['rmsd'] <= args.cutoff_correct_pose:
            data['label'] = 1
        else:
            data['label'] = 0
        node_labels.append(data['label'])

    if pdbid not in pdbids_test:
        pdbids_train_val.append(pdbid)
        if pkd is not None:
            # if pkd are provided, use them to construct training and validations sets
            labels_train_val.append(pkd[pdbid])
        else:
            labels_train_val.append(np.any(np.array(node_labels)==1).astype(int))

if args.nsplits >= 2:
    skf = StratifiedKFold(n_splits=args.nsplits, random_state=seed, shuffle=True)

elif args.nsplits == 1:
    skf = StratifiedShuffleSplit(n_splits=args.nsplits, train_size=args.fraction_train, random_state=seed)
else:
    raise ValueError("Number of splits should be greater or equal to 1")

if pkd is not None and args.binarize is None:
    groups = pd.cut(labels_train_val, 10, labels=False)
    splits = skf.split(np.zeros_like(groups), groups)
else:
    splits = skf.split(np.zeros_like(labels_train_val), labels_train_val)

datasets_list = []
for idxs_train, idxs_val in splits:
    pdbids_train = sorted([pdbids_train_val[idx] for idx in idxs_train])
    pdbids_val = sorted([pdbids_train_val[idx] for idx in idxs_val])

    datasets_list.append({'train': pdbids_train, 'val': pdbids_val, 'test': pdbids_test})

for kdx, datasets in enumerate(datasets_list):
    correctness_ratio = []

    graphs_copy = copy.deepcopy(graphs) 
    for pdbid in pdbids:
        G = graphs_copy[pdbid]
    
        true_nodes = []
        false_nodes = []
        for node, data in G.nodes(data=True):
            if data['label'] == 1:
                true_nodes.append(node)
            else:
                false_nodes.append(node)

        # select maximum number of nodes
        if pdbid in datasets['train']:
            if args.max_nnodes is not None:
                if len(true_nodes) > args.max_nnodes:
                    true_nodes = random.sample(true_nodes, args.max_nnodes)
     
                if len(false_nodes) > args.max_nnodes:
                    false_nodes = random.sample(false_nodes, args.max_nnodes)

                discarded_nodes = list(set(G.nodes()) - set(true_nodes+false_nodes))
                G.remove_nodes_from(discarded_nodes)

            if false_nodes:
                correctness_ratio.append(len(true_nodes)*1./len(false_nodes))

    if args.nsplits >= 2:
        print('Split %i: training set: %i elements, validation set: %i elements' \
            %(kdx+1, len(datasets['train']), len(datasets['val'])))
    else:
        print('Training set: %i elements, validation set: %i elements'%(len(datasets['train']), len(datasets['val'])))

    print('Alpha coef. pose correctness: %.2f'%(1/(1+np.mean(correctness_ratio))))

    if pkd is not None and args.binarize is not None:
        nactives, ninactives = (0, 0)

        for pdbid in datasets['train']:
            if pkd[pdbid] == 1:
                nactives += 1
            else:
                ninactives += 1
        print('Alpha coef. actives/inactives: %.2f'%(1/(1+nactives*1./ninactives)))

    # normalize node and edge features
    if args.normalize:
        rmsd = []
        scores = {sf: [] for sf in scoring_functions}

        for pdbid in datasets['train']:
            G = graphs_copy[pdbid]

            for node, data in G.nodes(data=True):
                for ft, value in data.items():
                    if ft in scoring_functions:
                        scores[ft].append(value)

            rmsd_graph = np.array([data['rmsd'] for _, _, data in G.edges(data=True)])
            rmsd.append(rmsd_graph)

        if args.nsplits >= 2:
            scaler_file = 'scaler_%i.pkl'%(kdx+1)
        else:
            scaler_file = 'scaler.pkl'

        scalers = {}
        for sf in scoring_functions:
            scaler = StandardScaler() 
            scaler.fit(np.array(scores[sf]).reshape(-1, 1))
            scalers[sf] = scaler

        with open(scaler_file, "wb") as ff:
            pickle.dump(scalers, ff)

        for pdbid in graphs_copy:
            G = graphs_copy[pdbid]

            for node, data in G.nodes(data=True):
                # normalize scores
                for sf in scoring_functions:
                    sf_array = np.array([[data[sf]]])
                    normalized_score = scalers[sf].transform(sf_array)
                    G.nodes[node][sf] = normalized_score[0][0]

    # save graphs and related information
    for setname, dataset_pdbids in datasets.items():
        dataset_graphs = []

        for jdx, pdbid in enumerate(dataset_pdbids):
            if pdbid in graphs_copy:
                G = graphs_copy[pdbid]

                if jdx == 0:
                    if args.nsplits >= 2:
                        infofile = open('info_'+setname+'_%i.csv'%(kdx+1), 'w')
                    else:
                        infofile = open('info_'+setname+'.csv', 'w')

                    if pkd is not None and args.binarize is None:
                        infofile.write("pdbid,nposes,ngraphs,ncorrect,nincorrect,pKd\n")
                    elif pkd is not None:
                        infofile.write("pdbid,nposes,ngraphs,ncorrect,nincorrect,active/inactive\n")
                    else:
                        infofile.write("pdbid,nposes,ngraphs,ncorrect,nincorrect\n")
    
                nposes = len(G)
                subgraphs = list(nx.connected_components(G))
                ngraphs = len(subgraphs)

                node_labels = [label for node, label in list(G.nodes(data='label'))]
                ncorrect = node_labels.count(1)
                nincorrect = node_labels.count(0)

                if pkd is not None:
                    dataset_graphs.append([G, pkd[pdbid]])
                    if args.binarize is None:
                        infofile.write("%s,%i,%i,%i,%i,%.2f\n"%(pdbid, nposes, ngraphs, ncorrect, nincorrect, pkd[pdbid]))
                    else:
                        infofile.write("%s,%i,%i,%i,%i,%i\n"%(pdbid, nposes, ngraphs, ncorrect, nincorrect, pkd[pdbid]))
                else:
                    infofile.write("%s,%i,%i,%i,%i\n"%(pdbid, nposes, ngraphs, ncorrect, nincorrect))
                    dataset_graphs.append(G)
    
                if jdx == len(dataset_pdbids)-1:
                    infofile.close()

        if args.nsplits >= 2:  
            filename = setname + '_%i.pkl'%(kdx+1)
        else:
            filename = setname + '.pkl'

        if dataset_pdbids:
            with open(filename, "wb") as ff:
                pickle.dump(dataset_graphs, ff)
