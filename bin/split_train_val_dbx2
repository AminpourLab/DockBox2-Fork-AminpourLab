#!/usr/bin/env python
import os
import sys
import pickle
import copy

import argparse
import random
import pandas as pd

import numpy as np
import networkx as nx

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import StratifiedKFold
from dockbox2.dbxconfig import known_scoring_functions, default_features

parser = argparse.ArgumentParser(description="split train and validation sets for training with DockBox2")

parser.add_argument('-g',
    dest='pkfile',
    required=True,
    help="pickle input file containing all the graphs")

parser.add_argument('--cutoff',
    dest='cutoff_correct_pose',
    metavar='FLOAT',
    default=2.0,
    type=float,
    help='RMSD cutoff used to assign correctness label')

parser.add_argument('--cv',
    dest='cross_validation',
    action="store_true",
    default=False,
    help='perform cross-validation (related options: nsplits)')

parser.add_argument('--normalize',
    dest='normalize',
    action="store_true",
    default=False,
    help='normalize known scoring functions')

parser.add_argument('--nsplits',
    dest='nsplits',
    metavar='INT',
    default=5,
    type=int,
    help='number of splits used for cross-validation')

parser.add_argument('--maxgraphs',
    dest='max_ngraphs',
    metavar='INT',
    type=int,
    help='maximum number of graphs used for both training and validation')

parser.add_argument('--maxnodes',
    dest='max_nnodes',
    metavar='INT',
    type=int,
    help='maximum number of poses per graph in training set')

parser.add_argument('--test',
    dest='value_test',
    metavar='FLOAT/STR',
    help='fraction assigned to test set or filename including IDs')

parser.add_argument('--train',
    dest='fraction_train',
    metavar='FLOAT',
    default=0.5,
    type=float,
    help='fraction assigned to train set (used when cv is disabled)')

parser.add_argument('--seed',
    dest='seed',
    default=None,
    metavar='INT',
    type=int,
    help='random seed')

# update parser with arguments
args = parser.parse_args()

if args.seed is not None:
    seed = args.seed
else:
    seed = random.randint(0, 1e10)
    print("The random seed is set to %i"%seed)

# set random seed
random.seed(seed)

def is_float(string):
    try:
        float(string)
        return True
    except ValueError:
        return False

with open(args.pkfile, "rb") as ff:
    graphs = pickle.load(ff)

# get PDBIDs from dictionnary keys
pdbids = sorted(list(set(graphs.keys())))
npdbids = len(pdbids)

if all([isinstance(graphs[pdbid], list) and len(graphs[pdbid])==2 for pdbid in pdbids]):
    pkd = {pdbid: graphs[pdbid][1] for pdbid in pdbids} 
    graphs = {pdbid: graphs[pdbid][0] for pdbid in pdbids}
    print("pKd's values found...")
else:
    pkd = None

# get names of scoring functions and other feats
for idx, pdbid in enumerate(pdbids):
    G = graphs[pdbid]
    for node, data in G.nodes(data=True):
        if idx == 0:
            data_names = data.keys()
            scoring_functions = [ft for ft in data if any(ft.startswith(sf) for sf in known_scoring_functions)]
            other_features = [ft for ft in data if ft in default_features]
        else:
            if any([key not in data_names for key in data]):
                print("Provided graphs are not consistent, %s data not found in every node"%key)  
                sys.exit(1)

# check restrictions on test set
if is_float(args.value_test):
    fraction_test = float(args.value_test*npdbids)
    ntest = int(args.fraction_test*npdbids)
    pdbids_test = sorted(random.sample(pdbids, ntest))
else:
    with open(args.value_test, 'r') as ff:
        pdbids_test = ff.readlines()
        pdbids_test = [pdbid.replace('\n', '') for pdbid in pdbids_test]
    pdbids_test = [pdbid for pdbid in pdbids if pdbid in pdbids_test]

if args.cross_validation:
    (pdbids_train_val, labels_train_val) = ([], [])

for pdbid in pdbids:
    G = graphs[pdbid]

    node_labels = []
    for node, data in G.nodes(data=True):
        # set label from cutoff_correct_pose
        if data['rmsd'] <= args.cutoff_correct_pose:
            data['label'] = 1
        else:
            data['label'] = 0
        node_labels.append(data['label'])

    if args.cross_validation and pdbid not in pdbids_test:
        pdbids_train_val.append(pdbid)
        if pkd is not None:
            # if pkd are provided, use them to construct training and validations sets
            labels_train_val.append(float(pkd[pdbid]))
        else:
            labels_train_val.append(np.any(np.array(node_labels)==1).astype(int))

if args.max_ngraphs:
    ratio_max_ngraphs = args.max_ngraphs*1./len(pdbids_train_val) 

datasets_list = []
if args.cross_validation:
    skf = StratifiedKFold(n_splits=args.nsplits, random_state=seed, shuffle=True)

    if pkd is not None:
        groups = pd.cut(labels_train_val, 10, labels=False)
        splits = skf.split(np.zeros_like(groups), groups)
    else:
        splits = skf.split(np.zeros_like(labels_train_val), labels_train_val)

    for idxs_train, idxs_val in splits:
        pdbids_train = [pdbids_train_val[idx] for idx in idxs_train]
        pdbids_val = [pdbids_train_val[idx] for idx in idxs_val]

        if args.max_ngraphs and ratio_max_ngraphs < 1.:
            ntrain = int(round(ratio_max_ngraphs*len(pdbids_train)))
            pdbids_train = sorted(random.sample(pdbids_train, ntrain))

            nval = int(round(ratio_max_ngraphs*len(pdbids_val)))
            pdbids_val = sorted(random.sample(pdbids_val, nval))

        datasets_list.append({'train': pdbids_train, 'val': pdbids_val, 'test': pdbids_test})
else:
    datasets = {'test': pdbids_test}

    ntrain = int(args.fraction_train*npdbids)
    datasets['train'] = sorted(random.sample(sorted(list(set(pdbids) - set(datasets['test']))), ntrain))
    datasets['val'] = sorted(list(set(pdbids) - set(datasets['train']) - set(datasets['test'])))

    if args.max_ngraphs and ratio_max_ngraphs < 1.:
        ntrain = int(round(ratio_max_ngraphs*len(datasets['train'])))
        datasets['train'] = sorted(random.sample(datasets['train'], ntrains))

        nval = int(round(ratio_max_ngraphs*len(datasets['val'])))
        datasets['val'] = sorted(random.sample(datasets['val'], nval))

    datasets_list.append(datasets)

for kdx, datasets in enumerate(datasets_list):
    correctness_ratio = []

    graphs_copy = copy.deepcopy(graphs) 
    for pdbid in pdbids:
        G = graphs_copy[pdbid]
    
        true_nodes = []
        false_nodes = []
        for node, data in G.nodes(data=True):
            if data['label'] == 1:
                true_nodes.append(node)
            else:
                false_nodes.append(node)

        # select maximum number of nodes
        if pdbid in datasets['train']:
            if args.max_nnodes is not None:
                if len(true_nodes) > args.max_nnodes:
                    true_nodes = random.sample(true_nodes, args.max_nnodes)
     
                if len(false_nodes) > args.max_nnodes:
                    false_nodes = random.sample(false_nodes, args.max_nnodes)

                discarded_nodes = list(set(G.nodes()) - set(true_nodes+false_nodes))
                G.remove_nodes_from(discarded_nodes)

            if false_nodes:
                correctness_ratio.append(len(true_nodes)*1./len(false_nodes))

    if args.cross_validation:
        print('split %i: training set: %i elements, validation set: %i elements'%(kdx+1, len(datasets['train']), len(datasets['val'])))
    else:
        print('training set: %i elements, validation set: %i elements'%(len(datasets['train']), len(datasets['val'])))
    print("alpha coefficient needed for balanced set: %.2f"%(1/(1+np.mean(correctness_ratio))))

    # normalize data
    if args.normalize:
        scores = {sf: [] for sf in scoring_functions}
        if all(ft in other_features for ft in ['instance', 'score']):
            normalize_original_scores = True
        else:
            normalize_original_scores = False

        for pdbid in datasets['train']:
            G = graphs_copy[pdbid]
            for node, data in G.nodes(data=True):
                for ft, value in data.items():
                    if ft in scoring_functions:
                        scores[ft].append(value)

        if args.cross_validation:
            scaler_file = 'scaler_%i.pkl'%(kdx+1)
        else:
            scaler_file = 'scaler.pkl'

        scalers = {}
        for sf in scoring_functions:
            scaler = StandardScaler() 
            scaler.fit(np.array(scores[sf]).reshape(-1, 1))
            scalers[sf] = scaler

        with open(scaler_file, "wb") as ff:
            pickle.dump(scalers, ff)

        for pdbid in graphs_copy:
            G = graphs_copy[pdbid]

            for node, data in G.nodes(data=True):
                # normalize scores
                for sf in scoring_functions:
                    sf_array = np.array([[data[sf]]])
                    normalized_score = scalers[sf].transform(sf_array)
                    G.nodes[node][sf] = normalized_score[0][0]

                # normalize original score
                if normalize_original_scores:
                    sf_array = np.array([[data['score']]])
                    normalized_score = scalers[data['instance']].transform(sf_array)
                    G.nodes[node]['score'] = normalized_score[0][0]

    # save graphs and related information
    for setname, dataset_pdbids in datasets.items():
        dataset_graphs = []

        for jdx, pdbid in enumerate(dataset_pdbids):
            if pdbid in graphs_copy:
                G = graphs_copy[pdbid]

                if jdx == 0:
                    if args.cross_validation:
                        infofile = open('info_'+setname+'_%i.csv'%(kdx+1), 'w')
                    else:
                        infofile = open('info_'+setname+'.csv', 'w')
                    if pkd is not None:
                        infofile.write("pdbid,nposes,ngraphs,ncorrect,nincorrect,pKd\n")
                    else:
                        infofile.write("pdbid,nposes,ngraphs,ncorrect,nincorrect\n")
    
                nposes = len(G)
                subgraphs = list(nx.connected_components(G))
                ngraphs = len(subgraphs)

                node_labels = [label for node, label in list(G.nodes(data='label'))]
                ncorrect = node_labels.count(1)
                nincorrect = node_labels.count(0)

                if pkd is not None:
                    infofile.write("%s,%i,%i,%i,%i,%.2f\n"%(pdbid, nposes, ngraphs, ncorrect, nincorrect, pkd[pdbid]))
                    dataset_graphs.append([G, pkd[pdbid]])
                else:
                    infofile.write("%s,%i,%i,%i,%i\n"%(pdbid, nposes, ngraphs, ncorrect, nincorrect))
                    dataset_graphs.append(G)
    
                if jdx == len(dataset_pdbids)-1:
                    infofile.close()

        if args.cross_validation:    
            filename = setname + '_%i.pkl'%(kdx+1)
        else:
            filename = setname + '.pkl'

        if dataset_pdbids:
            with open(filename, "wb") as ff:
                pickle.dump(dataset_graphs, ff)
