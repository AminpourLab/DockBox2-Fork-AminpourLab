#!/usr/bin/env python
import os
import sys
import argparse
import time
import random

os.environ['TF_CPP_MIN_LOG_LEVEL'] = str(2)
os.environ['PYTHONHASHSEED'] = str(2)

from dockbox2.datasets import *
from dockbox2 import dbxconfig

from dockbox2 import models
from dockbox2.utils import *

# command-line arguments and options
parser = argparse.ArgumentParser(description="Train DbX2 GNN model")

parser.add_argument('-f',
    dest='config_file',
    required=True,
    help='config file containing model parameters')

parser.add_argument('-t',
    type=str,
    dest='pkfiles_t',
    nargs='+',
    metavar='FILE',
    required=True,
    help='pickle file containing graphs relative to training set')

parser.add_argument('-v',
    type=str,
    dest='pkfiles_v',
    nargs='+',
    metavar='FILE',
    required=True,
    help='pickle file containing graphs relative validation set')

parser.add_argument('-p',
    dest='patience',
    default=3,
    metavar='INT',
    type=int,    
    help='patience used for early stopping')

parser.add_argument('--task',
    dest='task_level',
    default='node',
    choices=['node', 'graph'],
    required=False,
    help="task-level prediction: node (pose correctness) or graph (pKd values)")

parser.add_argument('-w',
    dest='h5file',
    default=None,
    required=False,
    help='h5file where to save parameters of trained model')

parser.add_argument('--seed',
    dest='seed',
    default=None,
    metavar='INT',
    type=int,
    help='random seed')

# update parsers with arguments
args = parser.parse_args()

if args.h5file is not None:
   if len(args.pkfiles_t) > 1:
       suffix, ext = os.path.splitext(args.h5file)
       h5file = [suffix + '_%i.h5'%(jdx+1) for jdx in range(len(args.pkfiles_t))]
   else:
       h5file = [args.h5file]

for jdx, (pkfile_train, pkfile_val) in enumerate(zip(args.pkfiles_t, args.pkfiles_v)):
    if args.seed is not None:
        seed = args.seed
    else:
        seed = random.randint(0, 1e10)
        print("random seed is set to %i"%seed)
    set_seed(args.seed)

    config = dbxconfig.ConfigSetup(args.config_file)
    
    depth = config.depth
    classifier = config.classifier
    nrof_neigh = config.nrof_neigh

    if jdx == 0:    
        config.pretty_print(task_level=args.task_level)

    # load data for training and validation sets
    train = GraphDataset(pkfile_train, config.node, config.edge, task_level=args.task_level)
    data_loader_train, data_slices_train = generate_data_loader(train, depth, nrof_neigh, **config.minibatch, randomize=True)

    valid = GraphDataset(pkfile_val, config.node, config.edge, task_level=args.task_level)
    data_loader_valid, data_slices_valid = generate_data_loader(valid, depth, nrof_neigh, **config.minibatch, randomize=True)

    model = models.GraphSAGE(train.nfeats, train.nlabels, depth, nrof_neigh, config.loss, config.aggregator, \
                config.classifier, config.readout, config.edge, attention_options=config.gat, task_level=args.task_level)
    model.build()

    if jdx == 0:
        model.summary()
    
    # set Adam optimizer
    optimizer_class = getattr(tf.keras.optimizers, 'Adam')
    optimizer = optimizer_class(learning_rate=tf.keras.optimizers.schedules.ExponentialDecay(**config.optimizer))
    
    best_loss = {'total_loss': 100}
    loc_patience = 0
    
    for epoch in range(config.epochs): 
        if loc_patience >= args.patience:
           print("\n'EarlyStopping' called!\n")
           break
    
        for kdx, data_loader in enumerate([data_loader_train, data_loader_valid]):
            labels, pred_labels, best_node_labels, pred_best_node_labels, is_correct_labels, graph_size = (None, None, None, None, None, None)
     
            for idx_batch, data_batch in enumerate(data_loader):
                if kdx == 0:
                    # train model from training set
                    with tf.GradientTape() as tape:
                        batch_labels, batch_pred_labels, batch_best_node_labels, batch_pred_best_node_labels, batch_is_correct_labels, batch_graph_size = \
                            model(*data_batch, training=True)
                        loss = model.call_loss(batch_labels, batch_pred_labels)

                    grads = tape.gradient(loss['total_loss'], model.trainable_weights)
                    optimizer.apply_gradients(zip(grads, model.trainable_weights))

                else:
                    # check predictions for validation set
                    batch_labels, batch_pred_labels, batch_best_node_labels, batch_pred_best_node_labels, batch_is_correct_labels, batch_graph_size = \
                            model(*data_batch, training=False)
     
                is_first = True if idx_batch == 0 else False
                labels = append_batch_results(labels, batch_labels, first=is_first)
                pred_labels = append_batch_results(pred_labels, batch_pred_labels, first=is_first)
 
                if args.task_level == 'node':   
                    best_node_labels = append_batch_results(best_node_labels, batch_best_node_labels, first=is_first) 
                    pred_best_node_labels = append_batch_results(pred_best_node_labels, batch_pred_best_node_labels, first=is_first)

                    is_correct_labels = append_batch_results(is_correct_labels, batch_is_correct_labels, first=is_first)
                graph_size = append_batch_results(graph_size, batch_graph_size, first=is_first)

            loss_values = model.call_loss(labels, pred_labels)

            if args.task_level == 'node':
                # compute success rate
                success_rate = model.success_rate(best_node_labels, pred_best_node_labels, is_correct_labels)

            elif args.task_level == 'graph':
                pearson_coefficient = model.pearson(labels, pred_labels)

            # save results in .csv file
            saved_file_mode = "w" if epoch == 0 else "a"
            if len(args.pkfiles_t) > 1:
                suffix = '_' + str(jdx+1)
            else:
                suffix = ''

            results_csv = "results_train%s.csv"%suffix if kdx == 0 else "results_val%s.csv"%suffix
            with open(results_csv, saved_file_mode) as csvf:
                if epoch == 0:
                    if args.task_level == 'node':
                        csvf.write("epoch,success_rate," + ','.join(loss_values.keys())+"\n")
                    elif args.task_level == 'graph':
                        csvf.write("epoch,pearson," + ','.join(loss_values.keys())+"\n")

                if args.task_level == 'node':
                    csvf.write(str(epoch) + ',' + ','.join(['%.5f'%value \
                                  for value in {'success': success_rate, **loss_values}.values()])+'\n')
                elif args.task_level == 'graph':
                    csvf.write(str(epoch) + ',' + ','.join(['%.5f'%value \
                                  for value in {'pearson': pearson_coefficient, **loss_values}.values()])+'\n')

            if kdx == 1:
                # check if loss value has decreased
                if loss_values['total_loss'] < best_loss['total_loss']:
                    best_loss = loss_values
                    loc_patience = 0

                    best_labels = labels
                    best_pred_labels = pred_labels
                else:
                    loc_patience += 1
        time.sleep(1)

    # save predicted labels for validation set
    save_predicted_labels("preds_val%s.pkl"%suffix, best_labels, best_pred_labels, graph_size, data_slices_valid)

    if args.h5file is not None:
        model.save_weights_h5(h5file[jdx])
