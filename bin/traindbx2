#!/usr/bin/env python
import os
import sys
import argparse
import time
import random

os.environ['TF_CPP_MIN_LOG_LEVEL'] = str(2)
os.environ['PYTHONHASHSEED'] = str(2)

from dockbox2.datasets import *
from dockbox2 import dbxconfig

from dockbox2 import models
from dockbox2.utils import *

# command-line arguments and options
parser = argparse.ArgumentParser(description="Train DbX2 GNN model")

parser.add_argument('-f',
    dest='config_file',
    required=True,
    help='config file containing model parameters')

parser.add_argument('-t',
    type=str,
    dest='pkfiles_t',
    nargs='+',
    metavar='FILE',
    required=True,
    help='pickle file containing graphs relative to training set')

parser.add_argument('-v',
    type=str,
    dest='pkfiles_v',
    nargs='+',
    metavar='FILE',
    required=True,
    help='pickle file containing graphs relative validation set')

parser.add_argument('-p',
    dest='patience',
    default=3,
    metavar='INT',
    type=int,    
    help='patience used for early stopping')

parser.add_argument('--task',
    dest='task_level',
    default=['node'],
    nargs='+',
    required=False,
    help="task-level prediction: node (pose correctness) and/or graph (pKd values)")

parser.add_argument('-w',
    dest='h5file',
    default=None,
    required=False,
    help='h5file where to save parameters of trained model')

parser.add_argument('--seed',
    dest='seed',
    default=None,
    metavar='INT',
    type=int,
    help='random seed')

# update parsers with arguments
args = parser.parse_args()

# check task level
task_level = list(dict.fromkeys(args.task_level))
task_level = sorted(task_level, reverse=True)

if all(task not in task_level for task in ['node', 'graph']):
    raise ValueError("Task level should be node and/or graph")

if args.h5file is not None:
   if len(args.pkfiles_t) > 1:
       suffix, ext = os.path.splitext(args.h5file)
       h5file = [suffix + '_%i.h5'%(jdx+1) for jdx in range(len(args.pkfiles_t))]
   else:
       h5file = [args.h5file]

for jdx, (pkfile_train, pkfile_val) in enumerate(zip(args.pkfiles_t, args.pkfiles_v)):
    if args.seed is not None:
        seed = args.seed
    else:
        seed = random.randint(0, 1e10)
        print("random seed is set to %i"%seed)
    set_seed(args.seed)

    config = dbxconfig.ConfigSetup(args.config_file, pkfile_train)
    
    depth = config.depth
    classifier = config.classifier
    nrof_neigh = config.nrof_neigh

    use_edger = config.use_edger
    weighting = config.weighting

    if jdx == 0:    
        config.pretty_print(task_level=task_level)

    # load data for training and validation sets
    train = GraphDataset(pkfile_train, config.node, config.edger, task_level, training=True)
    data_loader_train, data_slices_train = generate_data_loader(train, depth, nrof_neigh, **config.minibatch, randomize=True)

    valid = GraphDataset(pkfile_val, config.node, config.edger, task_level, training=True)
    data_loader_valid, data_slices_valid = generate_data_loader(valid, depth, nrof_neigh, **config.minibatch, randomize=True)

    model = models.GraphSAGE(train.nfeats, train.nlabels, depth, nrof_neigh, use_edger, config.loss, config.aggregator, \
        config.classifier, config.readout, config.node, attention_options=config.gat, edger_options=config.edger, task_level=task_level, weighting=weighting)
    model.build()

    if jdx == 0:
        model.summary()
    
    # set Adam optimizer
    optimizer_class = getattr(tf.keras.optimizers, 'Adam')
    optimizer = optimizer_class(learning_rate=tf.keras.optimizers.schedules.ExponentialDecay(**config.optimizer))
    
    best_loss = {'total_loss': 100}
    loc_patience = 0
    
    for epoch in range(config.epochs): 
        if loc_patience >= args.patience:
           print("\n'EarlyStopping' called!\n")
           break
    
        for kdx, data_loader in enumerate([data_loader_train, data_loader_valid]):
            node_labels, pred_node_labels, best_node_labels, pred_best_node_labels, is_correct_labels, graph_labels, pred_graph_labels, \
                graph_size = (None, None, None, None, None, None, None, None)
     
            for idx_batch, data_batch in enumerate(data_loader):
                if kdx == 0:
                    # train model on training set
                    with tf.GradientTape() as tape:
                        batch_node_labels, batch_pred_node_labels, batch_best_node_labels, batch_pred_best_node_labels, batch_is_correct_labels, \
                            batch_graph_labels, batch_pred_graph_labels, batch_graph_size = model(*data_batch, training=True)

                        loss = model.call_loss(batch_node_labels, batch_pred_node_labels, batch_graph_labels, batch_pred_graph_labels)
 
                    grads = tape.gradient(loss['total_loss'], model.trainable_weights)
                    optimizer.apply_gradients(zip(grads, model.trainable_weights))
                else:
                    # check predictions on validation set
                    batch_node_labels, batch_pred_node_labels, batch_best_node_labels, batch_pred_best_node_labels, batch_is_correct_labels, \
                            batch_graph_labels, batch_pred_graph_labels, batch_graph_size = model(*data_batch, training=False)

                is_first = True if idx_batch == 0 else False
 
                if 'node' in task_level:
                    node_labels = append_batch_results(node_labels, batch_node_labels, first=is_first)
                    pred_node_labels = append_batch_results(pred_node_labels, batch_pred_node_labels, first=is_first)

                    best_node_labels = append_batch_results(best_node_labels, batch_best_node_labels, first=is_first) 
                    pred_best_node_labels = append_batch_results(pred_best_node_labels, batch_pred_best_node_labels, first=is_first)

                    is_correct_labels = append_batch_results(is_correct_labels, batch_is_correct_labels, first=is_first)

                if 'graph' in task_level:
                    graph_labels = append_batch_results(graph_labels, batch_graph_labels, first=is_first)
                    pred_graph_labels = append_batch_results(pred_graph_labels, batch_pred_graph_labels, first=is_first)

                graph_size = append_batch_results(graph_size, batch_graph_size, first=is_first)

            loss_values = model.call_loss(node_labels, pred_node_labels, graph_labels, pred_graph_labels)

            if 'node' in task_level:
                # compute success rate
                success_rate = model.success_rate(best_node_labels, pred_best_node_labels, is_correct_labels)

            if 'graph' in task_level:
                pearson_coefficient = model.pearson(graph_labels, pred_graph_labels)
                r_squared_value = model.r_squared_value(graph_labels, pred_graph_labels)
                rmse = model.rmse(graph_labels, pred_graph_labels)

            # save results in .csv file
            saved_file_mode = "w" if epoch == 0 else "a"
            if len(args.pkfiles_t) > 1:
                suffix = '_' + str(jdx+1)
            else:
                suffix = ''

            results_csv = "results_train%s.csv"%suffix if kdx == 0 else "results_val%s.csv"%suffix
            with open(results_csv, saved_file_mode) as csvf:
                if epoch == 0:
                    columns = "epoch,"

                    if 'node' in task_level:
                        columns += "success_rate,"

                    if 'graph' in task_level:
                        columns += "rmse,pearson,r2,"
                    csvf.write(columns + ','.join(loss_values.keys())+"\n")    

                values = str(epoch) + ','
                if 'node' in task_level:
                    values += '%.5f,'%success_rate

                if 'graph' in task_level:       
                    values += '%.5f,%.5f,%.5f,'%(rmse, pearson_coefficient, r_squared_value)
                csvf.write(values + ','.join(['%.5f'%value for value in loss_values.values()])+'\n')

            if kdx == 1:
                # check if loss value has decreased
                if loss_values['total_loss'] < best_loss['total_loss']:
                    best_loss = loss_values
                    loc_patience = 0

                    node_labels_b = node_labels
                    pred_node_labels_b = pred_node_labels

                    if args.h5file is not None:
                        model.save_weights_h5(h5file[jdx])
                else:
                    loc_patience += 1
        time.sleep(1)

    # save predicted labels for validation set
    save_predicted_node_labels("preds_val%s.pkl"%suffix, node_labels_b, pred_node_labels_b, graph_size, data_slices_valid)
