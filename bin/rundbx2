#!/usr/bin/env python
import os
import sys
import argparse
import time

os.environ['AUTOGRAPH_VERBOSITY'] = '1'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

from dockbox2.datasets import *
from dockbox2 import dbxconfig
from dockbox2 import models
from dockbox2.utils import *

# command-line arguments and options
parser = argparse.ArgumentParser(description="Train GNN model")

parser.add_argument('-t',
    type=str,
    dest='pickfile_train',
    metavar='FILE',
    required=True,
    help='pickle file containing graphs relative to training set')

parser.add_argument('-v',
    type=str,
    dest='pickfile_val',
    metavar='FILE',
    required=True,
    help='pickle file containing graphs relative validation set')

parser.add_argument('-f',
    dest='config_file',
    required=True,
    help='config file containing model parameters')

# update parsers with arguments
args = parser.parse_args()
config = dbxconfig.ConfigSetup(args.config_file)

depth = config.depth
activation = config.activation
nrof_neigh = config.nrof_neigh
edge_feature = config.edge_feature

# optimizer options
optimizer_type = 'Adam'
lr_schedule = {'initial_learning_rate': 1e-3, 'decay_steps': 1000, 'decay_rate': 0.99, 'staircase':True}

train = GraphDataset(args.pickfile_train, edge_feature=edge_feature)
data_loader_train = generate_data_loader(train, depth, nrof_neigh, **config.minibatch)

valid = GraphDataset(args.pickfile_val, edge_feature=edge_feature)
data_loader_valid = generate_data_loader(valid, depth, nrof_neigh, **config.minibatch)

model = models.GraphSAGE(train.nfeats, train.nlabels, activation, depth, nrof_neigh, \
    config.loss, config.aggregator, is_edge_feature=(False if edge_feature is None else True), gat_options=config.gat)

model.build()
model.summary()

# set optimizer
optimizer_class = getattr(tf.keras.optimizers, optimizer_type)
optimizer = optimizer_class(learning_rate=tf.keras.optimizers.schedules.ExponentialDecay(**lr_schedule))

for epoch in range(config.epochs): 
    for jdx, data_loader in enumerate([data_loader_train, data_loader_valid]):
        labels, pred_labels, graph_labels, pred_graph_labels, best_node_labels = (None, None, None, None, None)
 
        if jdx == 0:
            results_csv = "results_train.csv"
        else:
            results_csv = "results_val.csv"

        for idx, data_batch in enumerate(data_loader):

            with tf.GradientTape() as tape:
                batch_labels, batch_pred_labels, batch_graph_labels, batch_pred_graph_labels, batch_best_node_labels, loss = \
                    model(*data_batch, training=(True if jdx == 0 else False))
 
            if jdx == 0:
                grads = tape.gradient(loss['total_loss'], model.trainable_weights)
                optimizer.apply_gradients(zip(grads, model.trainable_weights))
 
            is_first = (True if idx == 0 else False)
            labels = append_batch_results(labels, batch_labels, first=is_first)
            pred_labels = append_batch_results(pred_labels, batch_pred_labels, first=is_first)
 
            graph_labels = append_batch_results(graph_labels, batch_graph_labels, first=is_first)

            pred_graph_labels = append_batch_results(pred_graph_labels, batch_pred_graph_labels, first=is_first)
            best_node_labels = append_batch_results(best_node_labels, batch_best_node_labels, first=is_first)
 
        loss_values = model.call_loss(labels, pred_labels, graph_labels, pred_graph_labels)
        metrics_values = model.call_metrics(labels, pred_labels)
        graph_metrics_values = model.call_metrics(graph_labels, pred_graph_labels, level='graph')
 
        success_rate = model.success_rate(graph_labels, pred_graph_labels, best_node_labels)
 
        if epoch == 0:
            saved_file_mode = "w"
        else:
            saved_file_mode = "a"

        with open(results_csv, saved_file_mode) as csvf:
            if epoch == 0:
                csvf.write("epoch," + ','.join(metrics_values.keys())+ ',' + ','.join(graph_metrics_values.keys()) + ",success_rate," \
                + ','.join(loss_values.keys())+"\n")
            csvf.write(str(epoch) + ',' + ','.join(['%.5f'%value \
                          for value in {**metrics_values, **graph_metrics_values, 'success': success_rate, **loss_values}.values()])+'\n')

    time.sleep(1)
