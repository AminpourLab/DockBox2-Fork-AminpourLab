#!/usr/bin/env python
import os
import sys
import argparse
import time
import random
import numpy as np

from dockbox2.datasets import *
from dockbox2 import dbxconfig

from dockbox2 import models
from dockbox2.utils import *

os.environ['TF_CPP_MIN_LOG_LEVEL'] = str(2)
os.environ['PYTHONHASHSEED'] = str(2)

import tensorflow as tf

# command-line arguments and options
parser = argparse.ArgumentParser(description="Run DBX2 GNN model")

parser.add_argument('-f',
    dest='config_file',
    required=True,
    metavar='FILE',
    help='config file containing model parameters')

parser.add_argument('-t',
    type=str,
    dest='pickfile',
    metavar='FILE',
    required=True,
    help='pickle file containing graphs to be tested')

parser.add_argument('-w',
    dest='h5file',
    default=None,
    required=True,
    metavar='FILE',
    help='h5 file containing trained weights')

parser.add_argument('-o',
    dest='output_file',
    required=False,
    metavar='FILE',
    default='dbx2.score',
    help='File containing with dbx2 scores are listed')

parser.add_argument('--pkd',
    dest='predict_pkd',
    action='store_true',
    default=False,
    help="Performs graph-level task prediction to predict pKd's")

parser.add_argument('--seed',
    dest='seed',
    default=None,
    metavar='INT',
    type=int,
    help='Random seed')

parser.add_argument('--with-labels',
    dest='with_labels',
    action='store_true',
    default=False,
    help="Load label values and compute success rates")

# update parsers with arguments
args = parser.parse_args()

if args.seed is not None:
    seed = args.seed
else:
    seed = random.randint(0, 1e10)
    print("random seed is set to %i"%seed)
set_seed(args.seed)

config = dbxconfig.ConfigSetup(args.config_file)

depth = config.depth
classifier = config.classifier
nrof_neigh = config.nrof_neigh

dataset = GraphDataset(args.pickfile, config.edge, predict_pkd=args.predict_pkd, use_labels=args.with_labels)
data_loader, data_slices = generate_data_loader(dataset, depth, nrof_neigh, **config.minibatch, randomize=False)

model = models.GraphSAGE(dataset.nfeats, dataset.nlabels, depth, nrof_neigh, config.loss, config.aggregator, \
    config.classifier, config.graph_pooling, config.edge, attention_options=config.gat, predict_pkd=args.predict_pkd)
model.build()
model.load_weights_h5(args.h5file)

labels, pred_labels, graph_labels, pred_graph_labels, best_node_labels, graph_size = (None, None, None, None, None, None)

for idx_batch, data_batch in enumerate(data_loader):
    # check predictions for validation set
    batch_labels, batch_pred_labels, batch_graph_labels, batch_pred_graph_labels, batch_best_node_labels, batch_graph_size = \
            model(*data_batch, training=False)

    is_first = True if idx_batch == 0 else False
    labels = append_batch_results(labels, batch_labels, first=is_first)
    pred_labels = append_batch_results(pred_labels, batch_pred_labels, first=is_first)

    graph_labels = append_batch_results(graph_labels, batch_graph_labels, first=is_first)

    pred_graph_labels = append_batch_results(pred_graph_labels, batch_pred_graph_labels, first=is_first)
    best_node_labels = append_batch_results(best_node_labels, batch_best_node_labels, first=is_first)
    graph_size = append_batch_results(graph_size, batch_graph_size, first=is_first)

# saving output predictions
graph_cumsize = np.insert(np.cumsum(graph_size), 0, 0)
with open(args.output_file, 'w') as outf:
    for kdx in range(len(graph_size)):
        if kdx != 0:
            outf.write('\n')

        graph_pred_labels = tf.gather(pred_labels, tf.range(graph_cumsize[kdx], graph_cumsize[kdx+1]))
        graph_labels_i = tf.gather(labels, tf.range(graph_cumsize[kdx], graph_cumsize[kdx+1]))

        for jdx, label in enumerate(graph_pred_labels):
            if args.with_labels:
                outf.write('%-8.5f %4i\n'%(label, graph_labels_i[jdx]))
            else:
                outf.write('%-8.5f\n'%label)

# printing success rate and loss if labels are provided
if args.with_labels:
    # compute success rate
    success_rate = model.success_rate(graph_labels, pred_graph_labels, best_node_labels)

    # call loss values
    loss_values = model.call_loss(labels, pred_labels, graph_labels, pred_graph_labels)
    print(','.join(['%.5f'%value for value in {'success': success_rate, **loss_values}.values()])+'\n')
