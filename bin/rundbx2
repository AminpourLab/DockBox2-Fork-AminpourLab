#!/usr/bin/env python
import os
import sys
import argparse
import time

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

from dockbox2.datasets import *
from dockbox2 import dbxconfig
from dockbox2 import models

# command-line arguments and options
parser = argparse.ArgumentParser(description="Train DockBox2 model")

parser.add_argument('-t',
    type=str,
    dest='pickfile_train',
    metavar='FILE',
    required=True,
    help='pickle file containing graphs relative to training set')

parser.add_argument('-v',
    type=str,
    dest='pickfile_val',
    metavar='FILE',
    required=True,
    help='pickle file containing graphs relative validation set')

parser.add_argument('-f',
    dest='config_file',
    required=True,
    help='config file containing model parameters')

# update parsers with arguments
args = parser.parse_args()
config = dbxconfig.ConfigSetup(args.config_file)

depth = config.depth
activation = config.activation
nrof_neigh = config.nrof_neigh
edge_feature = config.edge_feature

# optimizer options
optimizer_type = 'Adam'
lr_schedule = {'initial_learning_rate': 1e-2, 'decay_steps': 1000, 'decay_rate': 0.99, 'staircase':True}

train = GraphDataset(args.pickfile_train, edge_feature=edge_feature)
data_loader_train = generate_data_loader(train, depth, nrof_neigh, **config.minibatch)

valid = GraphDataset(args.pickfile_val, edge_feature=edge_feature)
data_loader_valid = generate_data_loader(valid, depth, nrof_neigh, **config.minibatch)

model = models.GraphSAGE(train.nfeats, train.nlabels, activation, depth, nrof_neigh, \
    config.loss, config.aggregator, is_edge_feature=(False if edge_feature is None else True), attention_options=config.attention)

model.build()
model.summary()

# set optimizer
optimizer_class = getattr(tf.keras.optimizers, optimizer_type)
optimizer = optimizer_class(learning_rate=tf.keras.optimizers.schedules.ExponentialDecay(**lr_schedule))

for epoch in range(config.epochs):
    print("Epoch %i"%epoch)

    for idx, data_batch in enumerate(data_loader_train):

        with tf.GradientTape() as tape:
            batch_labels, batch_pred_labels, loss = model(*data_batch, training=True)

        grads = tape.gradient(loss['total_loss'], model.trainable_weights)
        optimizer.apply_gradients(zip(grads, model.trainable_weights))

        if idx == 0:
            labels = batch_labels
            pred_labels = batch_pred_labels
        else:
            labels = tf.concat([labels, batch_labels], axis=0)
            pred_labels = tf.concat([pred_labels, batch_pred_labels], axis=0)

    loss_values = model.call_loss(labels, pred_labels)
    metrics_values = model.call_metrics(labels, pred_labels)

    print('Train: '+ ''.join(['%s: %.5f    ' % (key, value) for key, value in {**metrics_values, **loss_values}.items()]))

    for idx, data_batch in enumerate(data_loader_valid):

        with tf.GradientTape() as tape:
            labels, pred_labels, loss = model(*data_batch, training=False)

        if idx == 0:
            labels = batch_labels
            pred_labels = batch_pred_labels
        else:
            labels = tf.concat([labels, batch_labels], axis=0)
            pred_labels = tf.concat([pred_labels, batch_pred_labels], axis=0)

    loss_values = model.call_loss(labels, pred_labels)
    metrics_values = model.call_metrics(labels, pred_labels)

    print('Valid: ' + ''.join(['%s: %.5f    ' % (key, value) for key, value in metrics_values.items()]))
    time.sleep(1)
